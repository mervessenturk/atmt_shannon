INFO: COMMAND: train.py --data /tmp/srauss/atmt3/preprocessed --source-lang cz --target-lang en --src-tokenizer toy_example/data/models/spm_cz.model --tgt-tokenizer toy_example/data/models/spm_en.model --max-epoch 10 --batch-size 16 --max-length 64 --n-encoder-layers 2 --n-decoder-layers 2 --dim-embedding 256 --dim-feedforward-encoder 1024 --dim-feedforward-decoder 1024 --warmup_strategy none --ignore-checkpoints --no-save --log-file /data/srauss/atmt3_runs/warmup_none_e10/train.log
INFO: Arguments: {'cuda': False, 'data': '/tmp/srauss/atmt3/preprocessed', 'source_lang': 'cz', 'target_lang': 'en', 'src_tokenizer': 'toy_example/data/models/spm_cz.model', 'tgt_tokenizer': 'toy_example/data/models/spm_en.model', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'transformer', 'max_epoch': 10, 'clip_norm': 4.0, 'lr': 0.0003, 'warmup_strategy': 'none', 'warmup_steps': 4000, 'warmup_lr_factor': 0.1, 'patience': 3, 'max_length': 64, 'log_file': '/data/srauss/atmt3_runs/warmup_none_e10/train.log', 'save_dir': 'checkpoints_asg4', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': True, 'epoch_checkpoints': False, 'ignore_checkpoints': True, 'encoder_dropout': 0.0, 'decoder_dropout': 0.0, 'dim_embedding': 256, 'attention_heads': 8, 'dim_feedforward_encoder': 1024, 'dim_feedforward_decoder': 1024, 'max_seq_len': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 2, 'encoder_embed_path': None, 'decoder_embed_path': None, 'seed': 53010121}
INFO: Built a model with 9901888 parameters
INFO: Epoch 000: loss 5.933 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 25.3 | clip 1
INFO: Time to complete epoch 000 (training only): 5.59 seconds
INFO: Epoch 000: valid_loss 7.13 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.25e+03 | BLEU 0.034
INFO: Epoch 001: loss 4.679 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 18.57 | clip 1
INFO: Time to complete epoch 001 (training only): 5.43 seconds
INFO: Epoch 001: valid_loss 7.17 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.3e+03 | BLEU 0.195
INFO: Epoch 002: loss 4.166 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 19.4 | clip 1
INFO: Time to complete epoch 002 (training only): 5.31 seconds
INFO: Epoch 002: valid_loss 7.05 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.16e+03 | BLEU 0.257
INFO: Epoch 003: loss 3.698 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 20.2 | clip 0.9524
INFO: Time to complete epoch 003 (training only): 5.40 seconds
INFO: Epoch 003: valid_loss 7.14 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.26e+03 | BLEU 0.100
INFO: Epoch 004: loss 3.263 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 21.13 | clip 0.9524
INFO: Time to complete epoch 004 (training only): 5.62 seconds
INFO: Epoch 004: valid_loss 7.22 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.37e+03 | BLEU 0.189
INFO: Epoch 005: loss 2.848 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 22.16 | clip 0.9365
INFO: Time to complete epoch 005 (training only): 5.32 seconds
INFO: Epoch 005: valid_loss 7.3 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.49e+03 | BLEU 0.298
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Loading the best model for final evaluation on the test set
INFO: Loaded checkpoint checkpoints_asg4/checkpoint_last.pt
INFO: Test set results: BLEU 0.389
INFO: Final Test Set Results: BLEU 0.39
