INFO: COMMAND: train.py --data /tmp/srauss/atmt3/preprocessed --source-lang cz --target-lang en --src-tokenizer toy_example/data/models/spm_cz.model --tgt-tokenizer toy_example/data/models/spm_en.model --max-epoch 10 --batch-size 16 --max-length 64 --n-encoder-layers 2 --n-decoder-layers 2 --dim-embedding 256 --dim-feedforward-encoder 1024 --dim-feedforward-decoder 1024 --warmup_strategy constant --warmup_steps 60 --warmup_lr_factor 0.3 --ignore-checkpoints --no-save --log-file /data/srauss/atmt3_runs/warmup_constant_e10/train.log
INFO: Arguments: {'cuda': False, 'data': '/tmp/srauss/atmt3/preprocessed', 'source_lang': 'cz', 'target_lang': 'en', 'src_tokenizer': 'toy_example/data/models/spm_cz.model', 'tgt_tokenizer': 'toy_example/data/models/spm_en.model', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'transformer', 'max_epoch': 10, 'clip_norm': 4.0, 'lr': 0.0003, 'warmup_strategy': 'constant', 'warmup_steps': 60, 'warmup_lr_factor': 0.3, 'patience': 3, 'max_length': 64, 'log_file': '/data/srauss/atmt3_runs/warmup_constant_e10/train.log', 'save_dir': 'checkpoints_asg4', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': True, 'epoch_checkpoints': False, 'ignore_checkpoints': True, 'encoder_dropout': 0.0, 'decoder_dropout': 0.0, 'dim_embedding': 256, 'attention_heads': 8, 'dim_feedforward_encoder': 1024, 'dim_feedforward_decoder': 1024, 'max_seq_len': 128, 'n_encoder_layers': 2, 'n_decoder_layers': 2, 'encoder_embed_path': None, 'decoder_embed_path': None, 'seed': 702377217}
INFO: Built a model with 9901888 parameters
INFO: Epoch 000: loss 6.767 | lr 0.0001033 | num_tokens 15.75 | batch_size 15.87 | grad_norm 29.93 | clip 1
INFO: Time to complete epoch 000 (training only): 5.89 seconds
INFO: Epoch 000: valid_loss 7.38 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.6e+03 | BLEU 0.000
INFO: Epoch 001: loss 5.104 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 21.22 | clip 1
INFO: Time to complete epoch 001 (training only): 5.43 seconds
INFO: Epoch 001: valid_loss 7.2 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.35e+03 | BLEU 0.151
INFO: Epoch 002: loss 4.421 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 19.28 | clip 1
INFO: Time to complete epoch 002 (training only): 5.60 seconds
INFO: Epoch 002: valid_loss 7.2 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.34e+03 | BLEU 0.237
INFO: Epoch 003: loss 3.903 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 20.11 | clip 0.9841
INFO: Time to complete epoch 003 (training only): 5.44 seconds
INFO: Epoch 003: valid_loss 7.14 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.26e+03 | BLEU 0.475
INFO: Epoch 004: loss 3.422 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 21.37 | clip 0.9524
INFO: Time to complete epoch 004 (training only): 5.52 seconds
INFO: Epoch 004: valid_loss 7.25 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.41e+03 | BLEU 0.346
INFO: Epoch 005: loss 2.979 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 22.23 | clip 0.9365
INFO: Time to complete epoch 005 (training only): 5.38 seconds
INFO: Epoch 005: valid_loss 7.47 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.76e+03 | BLEU 0.373
INFO: Epoch 006: loss 2.569 | lr 0.0003 | num_tokens 15.75 | batch_size 15.87 | grad_norm 24.76 | clip 0.9365
INFO: Time to complete epoch 006 (training only): 5.74 seconds
INFO: Epoch 006: valid_loss 7.38 | num_tokens 20.7 | batch_size 100 | valid_perplexity 1.61e+03 | BLEU 0.489
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Loading the best model for final evaluation on the test set
INFO: Loaded checkpoint checkpoints_asg4/checkpoint_last.pt
INFO: Test set results: BLEU 0.493
INFO: Final Test Set Results: BLEU 0.49
